{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "563GG-I27f9x"
      },
      "source": [
        "This notebook implements a version of Facebook's Wav2Letter Model, described in https://arxiv.org/pdf/1609.03193.pdf. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P1B7nhP7mI1"
      },
      "source": [
        "# Step 1: Import necessary libraries and download + visualize the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.1: Install libraries and import things\n"
      ],
      "metadata": {
        "id": "9jHc7xO583TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "8cM8BPNxJT3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qucMdzXkr2pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKLd_MWr4xDf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchaudio\n",
        "from torchaudio.datasets.librispeech import LIBRISPEECH\n",
        "import torchaudio.transforms as transforms\n",
        "from torchaudio.models.decoder import ctc_decoder \n",
        "from torchaudio.models import wav2letter\n",
        "\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.modules.loss import CTCLoss\n",
        "\n",
        "from torchsummary import summary\n",
        "from jiwer import wer # https://pypi.org/project/jiwer/\n",
        "from Levenshtein import distance # https://maxbachmann.github.io/Levenshtein/levenshtein.html#distance\n",
        "\n",
        "from datetime import datetime\n",
        "from enum import unique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrTGm0JL66Zl"
      },
      "outputs": [],
      "source": [
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "# Detect if we have a GPU available\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.2: Global Variables"
      ],
      "metadata": {
        "id": "Qc58_BYY9Cwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH=\"/content/drive/MyDrive/EECS 442 Final Project: Wav2Letter/Code/\"\n",
        "LEXICON_PATH = PATH + \"/lexicon.txt\"\n",
        "TOKENS_PATH = PATH + \"/tokens.txt\"\n",
        "COMPLETE_DATASET_PATH = PATH + \"/librispeech\"\n",
        "TRAIN_SET_PATH = PATH + \"/train_set\"\n",
        "TEST_SET_PATH = PATH + \"/test_set\"\n",
        "\n",
        "RERUN_DATALOADS = True; \n",
        "\"\"\"\n",
        "Other Global Variables defined elsewhere:\n",
        "  DEVICE: \"cuda:0\" or \"cpu\", defined in 1.1\n",
        "  COMPLETE_DATASET: original Librispeech dataset\n",
        "  TRAIN_SET: training set ready to pass into model\n",
        "  TEST_SET: test set ready to pass into model\n",
        "\"\"\";"
      ],
      "metadata": {
        "id": "rjfTZICy3P4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.3: Download Librispeech Dataset "
      ],
      "metadata": {
        "id": "IcQQMNEl9JAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY5ddwXZKJ6H"
      },
      "outputs": [],
      "source": [
        "# Download the Librispeech dataset (dev-clean portion)\n",
        "if not os.path.exists(COMPLETE_DATASET_PATH):\n",
        "  os.mkdir(COMPLETE_DATASET_PATH)\n",
        "\n",
        "COMPLETE_DATASET = torchaudio.datasets.LIBRISPEECH(COMPLETE_DATASET_PATH, url='dev-clean', download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.4: Define utility functions for visualizing data"
      ],
      "metadata": {
        "id": "itr0fB2v9TCM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCVbyn_R3O9D"
      },
      "outputs": [],
      "source": [
        "# These functions are from https://pytorch.org/audio/stable/tutorials/audio_datasets_tutorial.html \n",
        "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "    \"\"\" \n",
        "    Given an audio waveform ([n_channels, time] Torch tensor) and a sample rate, plots \n",
        "    a spectrogram representing the audio. \n",
        "    \"\"\"\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, _ = waveform.shape\n",
        "\n",
        "    figure, axes = plt.subplots(num_channels, 1)\n",
        "    if num_channels == 1:\n",
        "        axes = [axes]\n",
        "    for c in range(num_channels):\n",
        "        axes[c].specgram(waveform[c], Fs=sample_rate)\n",
        "        if num_channels > 1:\n",
        "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
        "        if xlim:\n",
        "            axes[c].set_xlim(xlim)\n",
        "    figure.suptitle(title)\n",
        "    plt.show(block=False)\n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "    \"\"\"\n",
        "    Given a raw audio file (represented as a [1, time] torch tensor) with the \n",
        "    given sample rate, creates an interactive, playable object that enables \n",
        "    user to play the audio file. \n",
        "    \"\"\"\n",
        "    waveform = waveform.numpy()\n",
        "\n",
        "    num_channels, _ = waveform.shape\n",
        "    if num_channels == 1:\n",
        "        display(Audio(waveform[0], rate=sample_rate))\n",
        "    elif num_channels == 2:\n",
        "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "    else:\n",
        "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
        "\n",
        "def print_sample(idx: int, spectrogram=False):\n",
        "  \"\"\"\n",
        "  Given an index that refers to the original dataset, plays the corresponding \n",
        "  audio file and prints the transcript. \n",
        "\n",
        "  Relies on global variable COMPLETE_DATASET\n",
        "  \"\"\"\n",
        "  sample = COMPLETE_DATASET[idx]\n",
        "  waveform = sample[0]\n",
        "  sample_rate = sample[1]\n",
        "  transcript = sample[2]\n",
        "\n",
        "  print(\"Sample %d from %s:\" % (idx, COMPLETE_DATASET_PATH))\n",
        "  play_audio(waveform, sample_rate)\n",
        "  print(transcript)\n",
        "\n",
        "  if(spectrogram):\n",
        "    plot_specgram(waveform, sample_rate, title=\"Spectrogram For Sample %d\" % idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.5: Define utility functions for retrieving lexicon and tokens in dataset"
      ],
      "metadata": {
        "id": "FvaS0O7k9dDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lexiconToLexFile(lexicon, path):\n",
        "  \"\"\"\n",
        "  Given a list of words, creates a lexicon file at path from those words. \n",
        "\n",
        "  Each line of the file contains a word, followed by a space-separated spelling\n",
        "  that is terminated by the \"silence\" character, which is designated as a pipe |. \n",
        "\n",
        "  For instance:\n",
        "      a a |\n",
        "      able a b l e |\n",
        "      about a b o u t |\n",
        "      ...\n",
        "  \"\"\"\n",
        "  f = open(path, \"w\")\n",
        "  for word in lexicon:\n",
        "    f.write(word + \" \")\n",
        "\n",
        "    for letter in word:\n",
        "      f.write(letter + \" \")\n",
        "\n",
        "    f.write(\"|\\n\")\n",
        "  \n",
        "  f.close()\n",
        "\n",
        "def tokensToTokenFile(tokens, path):\n",
        "  \"\"\"\n",
        "  Given a list of tokens, puts them into a token file located at path.\n",
        "\n",
        "  Tokens will each be on a separate line. The space token will be replaced by \n",
        "  the silence token, represented by a pipe (|). \n",
        "\n",
        "  The blank token (-) used by CTC decoding will also be included \n",
        "  \"\"\"\n",
        "  f = open(path, \"w\")\n",
        "  for token in tokens:\n",
        "    if(token == ' '):\n",
        "      f.write(\"|\\n\")\n",
        "    else:\n",
        "      f.write(token + \"\\n\")\n",
        "  f.close()\n",
        "\n",
        "def parseDataset(dataset, n_samples=10000):\n",
        "  tokens = set()\n",
        "  lexicon = set()\n",
        "  len_sum = 0; \n",
        "\n",
        "  n_samples = min(n_samples, len(dataset))\n",
        "\n",
        "  for i in range(n_samples):\n",
        "    item = dataset[i]\n",
        "    len_sum += item[0].shape[1] / item[1]\n",
        "\n",
        "    word_arr = item[2].split(' ')\n",
        "    for word in word_arr:\n",
        "      lexicon.add(word)\n",
        "    for c in item[2]:\n",
        "      tokens.add(c)\n",
        "\n",
        "    if(i % 100 == 0):\n",
        "      print(i)\n",
        "\n",
        "  average_len = len_sum / len(dataset)\n",
        "\n",
        "  return tokens, lexicon, average_len"
      ],
      "metadata": {
        "id": "q5343yKNP7o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.6: Visualize dataset and create lexicon, token files"
      ],
      "metadata": {
        "id": "-4wJ6Wp698XT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntRn1cS53ULv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The original Dataset is an array of tuples, each like:\n",
        "    waveform:     Tensor [1, time]  audio\n",
        "    sample_rate:  int               sampling rate, usually 16000\n",
        "    utterance:    str               transcript\n",
        "    speaker_id:   int     \n",
        "    chapter_id:   int\n",
        "    utterance_id: int\n",
        "https://pytorch.org/audio/stable/generated/torchaudio.datasets.LIBRISPEECH.html#librispeech \n",
        "\"\"\"\n",
        "\n",
        "if(RERUN_DATALOADS):\n",
        "  tokens, lexicon, average_len = parseDataset(COMPLETE_DATASET)\n",
        "  tokens = np.sort(np.append(list(tokens), '-'))\n",
        "  lexicon = np.sort(list(lexicon))\n",
        "\n",
        "  print(\"%d audio samples retrieved from dataset with average length of %.3f seconds\" % (len(COMPLETE_DATASET), average_len))\n",
        "  print(\"%d unique characters appear in the transcriptions:\" % len(tokens))\n",
        "  print(tokens)\n",
        "  print(\"%d unique words appear in the transcriptions.\" % len(lexicon))\n",
        "\n",
        "  # Print a random sample from the dataset to visualize \n",
        "  idx = np.random.randint(0, len(COMPLETE_DATASET))\n",
        "  print_sample(idx, spectrogram=True)\n",
        "\n",
        "  #lexiconToLexFile(lexicon, LEXICON_PATH)\n",
        "  #tokensToTokenFile(tokens, TOKENS_PATH)\n",
        "else:\n",
        "  print(\"Dataloads not rerun. Set RERUN_DATALOADS option in 1.2 to rerun.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyUZqUTX7uaP"
      },
      "source": [
        "# Step 2: Preprocess data and put into DataLoaders "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op2LdL1qDweW"
      },
      "source": [
        "## Step 2.1: Define encoding from characters to class labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use 29 characters (tokens) in our encoding scheme, plus an additional blank character requried by the CTC Decoder. \n",
        "\n",
        "* 0: space (represented as a | for CTC decoding, but a space \n",
        "everywhere else)\n",
        "* 1: apostrophe '\n",
        "* 2-27: uppercase english letters (A - Z)\n",
        "* 28: blank character (-), not included here \n"
      ],
      "metadata": {
        "id": "yyKPUqpNFeLE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7wCh-4uDtKd"
      },
      "outputs": [],
      "source": [
        "def charToClass(c: str): \n",
        "  \"\"\"\n",
        "  Given a character c (represented as a string of length 1), returns the \n",
        "  corresponding class value 0 - 28. \n",
        "  \"\"\"\n",
        "  if(c == ' '):\n",
        "    return 0\n",
        "  elif(c == '\\''):\n",
        "    return 1\n",
        "  elif(c == '-'):\n",
        "    return 2\n",
        "  else:\n",
        "    return ord(c) - ord('A') + 3\n",
        "\n",
        "def classToChar(id: int):\n",
        "  \"\"\"\n",
        "  Given a class id (represented as an int), returns the corresponding character\n",
        "  as a string of length 1. \n",
        "  \"\"\"\n",
        "  if(id == 0):\n",
        "    return ' '\n",
        "  elif(id == 1):\n",
        "    return '\\''\n",
        "  elif(id == 2):\n",
        "    return '-'\n",
        "  else:\n",
        "    return chr(ord('A') + id - 3)\n",
        "\n",
        "def transcriptToTensor(transcript: str):\n",
        "  \"\"\"\n",
        "  Given a transcript (represented as a string), returns a one-dimensional \n",
        "  tensor representing the translation of the transcript into class values\n",
        "  \"\"\"\n",
        "  res = torch.zeros((len(transcript)), device=DEVICE, dtype=torch.int8)\n",
        "  for i in range(len(transcript)):\n",
        "    res[i] = charToClass(transcript[i])\n",
        "  \n",
        "  return res\n",
        "\n",
        "def tensorToTranscript(t: torch.Tensor, valid_len=None):\n",
        "  \"\"\"\n",
        "  Given a 1D Tensor representing a transcript, returns the corresponding string\n",
        "  by using classToChar to translate back into characters. \n",
        "  \"\"\"\n",
        "\n",
        "  if(not valid_len):\n",
        "    valid_len = t.shape[0]\n",
        "\n",
        "  res = \"\"\n",
        "  for i in range(valid_len):\n",
        "    res += classToChar(int(t[i]))\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh64W-r62svP"
      },
      "source": [
        "## Step 2.2: Define and Apply Wav Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.1: Define transformation functions"
      ],
      "metadata": {
        "id": "0zhTJAHGHNdh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM5DWAe-2r5P"
      },
      "outputs": [],
      "source": [
        "def splitAndTransformData(complete_dataset, train_split: float=0.3, sample_rate=16000, n_mfcc=13):\n",
        "  \"\"\"\n",
        "  Split data into train and test data sets while removing unnecessary data points\n",
        "  Transform audio data into 2D MFCC feature tensor [n_features, length/200]\n",
        "  Transform transcript data into 1D tensor representing each character as a class\n",
        "\n",
        "  Args:\n",
        "      complete_dataset: the complete librispeech dataset\n",
        "                        array of tuples in the form (waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id)\n",
        "                        where waveform is torch.Tensor of shape (1, num_sample_points)\n",
        "\n",
        "  Returns:\n",
        "      train_dataset: list of tuples in the form (mfccWaveform, intUtterance, originalIdx)\n",
        "      test_dataset: list of tuples in the form (mfccWaveform, intUtterance, originalIdx)\n",
        "  \"\"\"\n",
        "  train_dataset = []\n",
        "  test_dataset = []\n",
        "\n",
        "  split = int(train_split * len(complete_dataset))\n",
        "\n",
        "  MFCC_transformer = transforms.MFCC(sample_rate, n_mfcc)\n",
        "\n",
        "  # CPU tensors are used here because they need to interface with NumPy later\n",
        "  for i in range(len(complete_dataset)):\n",
        "    if i < split:\n",
        "      train_dataset.append((torch.squeeze(MFCC_transformer(complete_dataset[i][0])), \n",
        "                            transcriptToTensor(complete_dataset[i][2]).cpu(), i))\n",
        "    else:\n",
        "      test_dataset.append((torch.squeeze(MFCC_transformer(complete_dataset[i][0])),\n",
        "                           transcriptToTensor(complete_dataset[i][2]).cpu(), i))\n",
        "\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "def sortTimeData(unsorted: list):\n",
        "  \"\"\"\n",
        "  Sort data based on length of the audio sample\n",
        "\n",
        "  Args:\n",
        "      unsorted: list of tuples in the form (mfccWaveform, intUtterance, idx)\n",
        "                where mfccWaveform is torch.Tensor of shape (n_mfcc, sample_points)\n",
        "\n",
        "  Returns:\n",
        "      sorted: list of tuples in the form (mfccWaveform, intUtterance, idx) sorted on length of waveform\n",
        "  \"\"\"\n",
        "  sorted_out = sorted(unsorted, key=lambda waveform_length: waveform_length[0].shape[1], reverse=False)\n",
        "\n",
        "  return sorted_out\n",
        "\n",
        "\n",
        "def batchTimeData(dataset: list, batch_size: int = 64):\n",
        "  \"\"\"\n",
        "  Batch samples into bins\n",
        "\n",
        "  Args:\n",
        "    dataset: list of tuples in form (mfccWaveform, intUtterance, idx), sorted on waveform length\n",
        "    batch_size: int \n",
        "\n",
        "  Returns:\n",
        "    batched_dataset: list of batches, where each batch is a list of 3-tuples\n",
        "  \"\"\"\n",
        "  batched_array = np.array_split(np.asarray(dataset), np.arange(batch_size,len(dataset),batch_size))\n",
        "  batched_dataset = [batched.tolist() for batched in batched_array]\n",
        "\n",
        "  return batched_dataset\n",
        "\n",
        "\n",
        "def padTimeData(batch: list):\n",
        "  \"\"\"\n",
        "  Pad audio samples with 0s in the first dimension (time) until they are the \n",
        "    same dimensions as the longest audio sample in the batch.\n",
        "  Pad transcripts with 0s until they are the same length (in characters) as the \n",
        "    longest transcript in the batch\n",
        "\n",
        "  Args:\n",
        "    batch: list of tuples in form (mfccWaveform, intUtterance), sorted on waveform length\n",
        "           list should be batch_size elements long\n",
        "\n",
        "  Returns:\n",
        "    padded_batch: list of tuples in form (mfccWaveform: torch.Tensor [n_mfcc, MAX_LENGTH_OF_BATCH_AUDIO], \n",
        "                                          intUtterance: torch.Tensor [MAX_LENGTH_OF_BATCH_UTTERANCE],\n",
        "                                          origWaveformLength: int,\n",
        "                                          origUtteranceLength: int) \n",
        "                  of length batch_size. \n",
        "  \"\"\"\n",
        "\n",
        "  n_mfcc = batch[-1][0].shape[0]\n",
        "  max_audio_length = batch[-1][0].shape[1]\n",
        "  max_transcript_length = max(item[1].shape[0] for item in batch)\n",
        "\n",
        "  padded_batch = [(torch.cat((audio, torch.zeros((n_mfcc, max_audio_length - audio.shape[1]))), dim=1), \n",
        "                   torch.cat((caption, torch.zeros((max_transcript_length - caption.shape[0]))), dim=0),\n",
        "                   audio.shape[1], \n",
        "                   caption.shape[0], \n",
        "                   idx) for audio, caption, idx in batch]\n",
        "\n",
        "  return padded_batch\n",
        "\n",
        "\n",
        "def tensorizeDataset(regularized_dataset):\n",
        "  \"\"\"\n",
        "  Transform time samples into MFCC samples and randomize the order of MFCC samples\n",
        "\n",
        "  Args:\n",
        "    regularized_dataset: list of batches with batch_size elements\n",
        "                         each batch is a list of 4-tuples:\n",
        "                            audio: Tensor [1 x time]\n",
        "                            transcript: Tensor [time]\n",
        "                            origAudioLength: int \n",
        "                            origTranscriptLength: int\n",
        "    sample_rate: frequency of audio data in time (Hz), default = 16kHz\n",
        "    n_mfcc: number of MFCC Feature coefficients, default = 13\n",
        "\n",
        "  Returns:\n",
        "    mfcc_data: list of batch tensors of shape (batch_size, MFCC_coeffs, sample_length)\n",
        "  \"\"\"\n",
        "  # TODO: Update Docstring\n",
        "\n",
        "  final_data = [] # list of batches\n",
        "  n_mfcc = regularized_dataset[0][0][0].shape[0]\n",
        "\n",
        "  for batch in regularized_dataset:\n",
        "    idxs = np.arange(len(batch))\n",
        "    np.random.shuffle(idxs)\n",
        "\n",
        "    audio_tensor = torch.zeros((len(batch), n_mfcc, batch[0][0].shape[1]))\n",
        "    transcript_tensor = torch.zeros((len(batch), batch[0][1].shape[0]))\n",
        "    audio_len_tensor = torch.zeros((len(batch)), dtype=torch.int16)\n",
        "    transcript_len_tensor = torch.zeros((len(batch)), dtype=torch.int16)\n",
        "    idx_tensor = torch.zeros((len(batch)), dtype=torch.int16)\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "      data_idx = idxs[i]; \n",
        "      audio_tensor[i,:,:] = batch[data_idx][0]\n",
        "      transcript_tensor[i, :] = batch[data_idx][1]\n",
        "      audio_len_tensor[i] = batch[data_idx][2]\n",
        "      transcript_len_tensor[i] = batch[data_idx][3]\n",
        "      idx_tensor[i] = batch[data_idx][4]\n",
        "\n",
        "    final_data.append((audio_tensor, transcript_tensor, \n",
        "                      audio_len_tensor, transcript_len_tensor, idx_tensor))\n",
        "\n",
        "  return final_data\n",
        "\n",
        "\n",
        "def dataPreProcess(dataset_in, train_split : float = 0.70, batch_size : int = 64, n_mfcc: int = 13):\n",
        "  \"\"\"\n",
        "  Perform entirety of audio data pre-processing pipeline, this includes:\n",
        "    splitting test and train sets\n",
        "    sorting data\n",
        "    batching sorted data\n",
        "    padding sorted data\n",
        "    randomizing batch elements  \n",
        "\n",
        "    Args:\n",
        "      dataset_in: Complete dataset in\n",
        "      train_split: float, percentage of data to allocate to training set (default 0.3)\n",
        "      batch_size: int, number of audio samples per batch (default 64)\n",
        "\n",
        "    Returns:\n",
        "      final_train: batched, padded and randomized training dataset\n",
        "      final_test: batched, padded and randomized testing dataset\n",
        "  \"\"\"\n",
        "  train_set, test_set = splitAndTransformData(dataset_in, train_split)\n",
        "\n",
        "  sorted_train = sortTimeData(train_set)\n",
        "  sorted_test = sortTimeData(test_set)\n",
        "\n",
        "  batched_train = batchTimeData(sorted_train)\n",
        "  batched_test = batchTimeData(sorted_test)\n",
        "\n",
        "  padded_train = [padTimeData(train_batch) for train_batch in batched_train]\n",
        "  padded_test = [padTimeData(test_batch) for test_batch in batched_test]\n",
        "\n",
        "  final_train = tensorizeDataset(padded_train)\n",
        "  final_test = tensorizeDataset(padded_test)\n",
        "\n",
        "  return final_train, final_test\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.2: Define a verification function to ensure data translation is done correctly\n"
      ],
      "metadata": {
        "id": "BfTJ4hwBHUoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_random(dataset):\n",
        "  batch_idx = np.random.randint(0, len(dataset))\n",
        "  batch = dataset[batch_idx]\n",
        "  sample_idx = np.random.randint(0, batch[0].shape[0])\n",
        "  \n",
        "  stored_transcript = tensorToTranscript(batch[1][sample_idx])\n",
        "  stored_idx = batch[4][sample_idx]\n",
        "\n",
        "  print_sample(stored_idx)\n",
        "  print(\"Stored Index: %d\" % stored_idx)\n",
        "  print(\"Stored Transcript:\", stored_transcript)"
      ],
      "metadata": {
        "id": "DvIRdehlDneR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2.3: Apply data transforms to obtain final train and test set\n"
      ],
      "metadata": {
        "id": "SMVV5CgUHaMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SPLIT = 0.8\n",
        "BATCH_SIZE = 64\n",
        "N_MFCC = 13"
      ],
      "metadata": {
        "id": "n0GuWgMdC2PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes about 1 second for every 100 samples\n",
        "\n",
        "if(RERUN_DATALOADS):\n",
        "  train_set, test_set = dataPreProcess(COMPLETE_DATASET, train_split=TRAIN_SPLIT, batch_size=BATCH_SIZE, n_mfcc=N_MFCC)\n",
        "  torch.save(train_set, PATH + \"train_set\")\n",
        "  torch.save(test_set, PATH + \"test_set\")\n",
        "else:\n",
        "  print(\"Dataloads not rerun. Set RERUN_DATALOADS option in 1.2 to rerun.\")\n",
        "  train_set = torch.load(PATH + \"train_set\")\n",
        "  test_set = torch.load(PATH + \"test_set\")\n",
        "\n",
        "print(\"\\n\", \"------\"*20)\n",
        "print(\"Audio Shape (Sample):\", train_set[0][0].shape) #0th batch 0th sample audio shape\n",
        "print(\"Transcript Shape (Sample):\", train_set[0][1].shape) #0th batch 0th sample transcript shape\n",
        "print(\"Audio-Original-Length Shape:\", train_set[0][2].shape) #0th batch 0th sample audio-len shape\n",
        "print(\"Transcript-Original-Length Shape:\", train_set[0][3].shape) #0th batch 0th sample transcript-len shape\n",
        "print(\"Idx Shape:\", train_set[0][4].shape) #0th batch 0th sample transcript-len shape\n",
        "\n",
        "print(\"\\n\", \"------\"*20)\n",
        "verify_random(train_set)\n",
        "\n",
        "print(\"\\n\", \"------\"*20)\n",
        "verify_random(test_set)"
      ],
      "metadata": {
        "id": "BEQVr-B_RnP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7m2SCbf8C7i"
      },
      "source": [
        "# Step 3: Define model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXwQu3CeNAUi"
      },
      "source": [
        "##Step 3.1: Define network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFoy88EtQn0y"
      },
      "outputs": [],
      "source": [
        "class Wav442Letter(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, bottleneck_size=2000):\n",
        "    super(Wav442Letter, self).__init__()\n",
        "\n",
        "    # We may need to make this smaller depending on \n",
        "    self.network = nn.Sequential(\n",
        "        nn.Conv1d(in_channels, 250, kernel_size=48, stride=2, padding=23), # modified stride here to avoid padding\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(250, 250, kernel_size=7, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(250, 250, kernel_size=7, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(250, 250, kernel_size=7, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(250, 250, kernel_size=7, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(250, 250, kernel_size=7, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(250, 250, kernel_size=7, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(250, 250, kernel_size=7, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(250, bottleneck_size, kernel_size=32, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(bottleneck_size, bottleneck_size, kernel_size=1, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(bottleneck_size, out_channels, kernel_size=1, stride=1, padding='same'),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "  \n",
        "  # x should be of shape (batchSize, num_features, length)\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    logits = self.network(x)\n",
        "    log_probs = F.log_softmax(logits, dim=1)\n",
        "    return logits, log_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4-8c1wMVueW"
      },
      "outputs": [],
      "source": [
        "model = Wav442Letter(13, 29, bottleneck_size=500).to(\"cuda\")\n",
        "summary(model, (13, 400))\n",
        "temp = model(torch.zeros((5, 13, 400)).to(\"cuda\"))\n",
        "print(temp[0].shape, temp[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaD6GSUMUI_n"
      },
      "source": [
        "## Step 3.2: Define train function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See NVIDIA Implementation for hyperparamters: https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/wave2letter.html\n"
      ],
      "metadata": {
        "id": "JM6fuyzw0qf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List of hyperparameters:**\n",
        "\n",
        "* Epochs\n",
        "* Optimizer: SGD, Adam\n",
        "* Learning Rate (Initial)\n",
        "* Momentum\n",
        "* Weight Decay\n",
        "* Scheduler Patience\n",
        "* Scheduler Rate Factore\n",
        "* Scheduler Threshold\n",
        "\n",
        "\n",
        "**List of potential architecture changes:**\n",
        "* Stride in first layer (original 2)\n",
        "  * If changed to 1, need to change padding to 'same'\n",
        "  * Also need to modify train and eval functions for output size changes\n",
        "* Bottleneck size (Fully-Connected): 2000\n",
        "  * Can experiment going down to 500"
      ],
      "metadata": {
        "id": "UhEhI_2UQCO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.scale import LogitScale\n",
        "def train_model(model, dataset, epochs=1, ctc_loss=None, optimizer=None):\n",
        "\n",
        "  if(not ctc_loss): # define ctc_loss function\n",
        "    ctc_loss = CTCLoss(blank=2, reduction='mean', zero_infinity=False)\n",
        "\n",
        "  if(not optimizer): # define optimizer\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-4, momentum=0.9)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "  loss_by_batch = []\n",
        "  loss_by_epoch = []\n",
        "  score_by_batch = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    loss_epoch = 0; \n",
        "\n",
        "    for batch in dataset:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # T: input length\n",
        "      # N: batch size\n",
        "      # C: num_classes\n",
        "      # S: max target length\n",
        "      \n",
        "      logits, log_probs = model(batch[0].to(DEVICE))\n",
        "      logits = torch.permute(logits, (2, 0, 1)) # (N, C, T) --> (T, N, C)\n",
        "      log_probs = torch.permute(log_probs, (2, 0, 1))\n",
        "\n",
        "      targets = batch[1].to(DEVICE) # (N, S) \n",
        "      input_lens = batch[2].to(DEVICE) # (N)\n",
        "      target_lens = batch[3].to(DEVICE) # (N)\n",
        "\n",
        "      loss = ctc_loss(log_probs, targets, input_lens // 2, target_lens)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_by_batch.append(loss.detach());\n",
        "      loss_epoch += loss.detach().cpu()\n",
        "\n",
        "      # pred = decoder(log_probs[0].reshape((1, log_probs.shape[1], log_probs.shape[2])).to('cpu'), input_lens[0].reshape((1)).to('cpu'))\n",
        "      # if(len(pred[0])):\n",
        "      #   score_by_batch.append(pred[0][0].score)\n",
        "      #   print(pred[0][0].score)\n",
        "\n",
        "    #scheduler.step(loss_epoch)\n",
        "    print(\"Epoch %d Loss: %.3f\" % (epoch, loss_epoch))\n",
        "    loss_by_epoch.append(loss_epoch)\n",
        "\n",
        "    if(epoch % 100 == 0):\n",
        "      now = datetime.now()\n",
        "      dt_string = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
        "      torch.save(model, PATH + \"/model-full_\" + dt_string)\n",
        "      \n",
        "  return loss_by_batch, loss_by_epoch, score_by_batch"
      ],
      "metadata": {
        "id": "WFMICPasNzMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EUkcqO0Uikc"
      },
      "source": [
        "## Step 3.3: Define eval function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, dataset):\n",
        "  \"\"\"\n",
        "  Given a model and a (test) dataset, runs the dataset through the model and \n",
        "  decodes outputs using a CTC decoder. \n",
        "\n",
        "  Returns a list of 7-tuples, each tuple corresponding to a batch. \n",
        "  The first 5 elements of the tuple are the original 5 elements in a batch:\n",
        "    - mfcc waveform\n",
        "    - transcript as an int tensor\n",
        "    - mfcc valid length\n",
        "    - transcript valid length\n",
        "    - original index\n",
        "  The next two elements are two lists. \n",
        "    - ground truth transcripts, len = BATCH_SIZE\n",
        "    - predicted transcripts, len = BATCH_SIZE\n",
        "      - each element in predicted transcripts is a 3-tuple:\n",
        "        - pred transcript\n",
        "        - pred tokens\n",
        "        - score \n",
        "  \"\"\"\n",
        "  # there are hyperparams that can be set for this decoder, found here:\n",
        "  # https://pytorch.org/audio/master/generated/torchaudio.models.decoder.ctc_decoder.html#torchaudio.models.decoder.ctc_decoder \n",
        "  decoder = ctc_decoder(lexicon=LEXICON_PATH, tokens=TOKENS_PATH, blank_token='-', sil_token='|')\n",
        "  \n",
        "  eval_out = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    idx = 0\n",
        "    total_wer = 0\n",
        "    total_lev_dist = 0\n",
        "    for batch in dataset:\n",
        "      # T: input length\n",
        "      # N: batch size\n",
        "      # C: num_classes\n",
        "      logits, log_probs = model(batch[0].to(DEVICE))\n",
        "\n",
        "      logits = torch.permute(logits, (0, 2, 1)) # (N, C, T) --> (N, T, C)\n",
        "      probs = torch.permute(torch.exp(log_probs), (0, 2, 1))\n",
        "      log_probs = torch.permute(log_probs, (0, 2, 1))\n",
        "\n",
        "      targets = batch[1].to(DEVICE) # (N, S) \n",
        "      input_lens = batch[2].to(DEVICE) # (N)\n",
        "      target_lens = batch[3].to(DEVICE) # (N)\n",
        "\n",
        "      # List of List[CTCHypothesis]\n",
        "      # https://pytorch.org/audio/master/generated/torchaudio.models.decoder.CTCDecoder.html#torchaudio.models.decoder.CTCHypothesis \n",
        "      pred = decoder(logits.to('cpu'), (input_lens // 2).cpu())\n",
        "\n",
        "\n",
        "      batch_size = targets.shape[0]\n",
        "\n",
        "      gt_transcripts = []\n",
        "      pred_transcripts = []\n",
        "\n",
        "      for j in range(batch_size):\n",
        "        gt = tensorToTranscript(targets[j], target_lens[j])\n",
        "        gt_transcripts.append(gt)\n",
        "        \n",
        "        if(len(pred[j])):\n",
        "          transcript = \" \".join(pred[j][0].words).strip()\n",
        "          tokens = \"\".join(decoder.idxs_to_tokens(pred[j][0].tokens))\n",
        "          score = pred[j][0].score\n",
        "          word_err_rate = wer(gt, transcript)\n",
        "          lev_dist = distance(gt, transcript)\n",
        "          pred_transcripts.append((transcript, tokens, score, word_err_rate, lev_dist))\n",
        "\n",
        "          total_wer += word_err_rate\n",
        "          if(len(transcript)):\n",
        "            total_lev_dist += lev_dist / len(transcript)\n",
        "\n",
        "          idx += 1\n",
        "\n",
        "      eval_out.append((probs, targets, input_lens, target_lens, batch[4], gt_transcripts, pred_transcripts))\n",
        "      \n",
        "  return eval_out, total_wer / idx, total_lev_dist / idx\n"
      ],
      "metadata": {
        "id": "o__Jtjk7Cajw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Define Visualization Functions\n"
      ],
      "metadata": {
        "id": "U1ZXOysDvfH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.1: Define loss visualization"
      ],
      "metadata": {
        "id": "OHVD7N2JvLYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(loss_list, x_label=\"Epoch\"):\n",
        "  plt.figure()\n",
        "  plt.plot(loss_list)\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.xlabel(x_label)\n",
        "  plt.title(\"Loss over time\")\n",
        "  plt.xticks(range(0, len(loss_list), len(loss_list)//10 + 1))"
      ],
      "metadata": {
        "id": "F08hHVGJvslL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_eval(eval_out, n_samples=10):\n",
        "  random.shuffle(eval_out)\n",
        "\n",
        "  for sample_idx in range(n_samples):\n",
        "    batch_idx = np.random.randint(eval_out[sample_idx][0].shape[0])\n",
        "\n",
        "    log_probs = eval_out[sample_idx][0][batch_idx]\n",
        "    orig_idx = eval_out[sample_idx][4][batch_idx]\n",
        "    gt_transcript = eval_out[sample_idx][5][batch_idx]\n",
        "    pred_transcript = eval_out[sample_idx][6][batch_idx][0]\n",
        "    pred_tokens = eval_out[sample_idx][6][batch_idx][1]\n",
        "    pred_score = eval_out[sample_idx][6][batch_idx][2]\n",
        "    word_err_rate = eval_out[sample_idx][6][batch_idx][3]\n",
        "    lev_dist = eval_out[sample_idx][6][batch_idx][4]\n",
        "\n",
        "    pred_classes = [classToChar(c) for c in torch.argmax(log_probs, dim=1)]\n",
        "    pred_str = \"\"\n",
        "    pred_str = pred_str.join(pred_classes)\n",
        "\n",
        "    print(\"Sample %d\" % sample_idx)\n",
        "    print_sample(orig_idx)\n",
        "    print(\"GT:\", gt_transcript)\n",
        "    print(\"Pred:\", pred_transcript)\n",
        "    print(\"Pred Tokens:\", pred_tokens)\n",
        "    print(\"Raw Max Tokens:\", pred_str)\n",
        "    print(\"Score: %.3f\" % pred_score)\n",
        "    print(\"Word Error Rate: %.3f\" % word_err_rate)\n",
        "    print(\"Levenstein Distance: %.3f\" % lev_dist)\n",
        "\n",
        "    plt.figure(dpi=300)\n",
        "    plt.imshow(torch.permute(log_probs, (1, 0)).cpu(), cmap='viridis', interpolation='nearest')"
      ],
      "metadata": {
        "id": "d1p2q-poxaUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3-yk86MUNTQ"
      },
      "source": [
        "# Step 4: Train model and visualize loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD_PATH = PATH + \"/model-weights_2022-12-09_11-14\"\n",
        "# model = Wav442Letter(13, 29, bottleneck_size=500).to(DEVICE)\n",
        "#model.load_state_dict(torch.load(LOAD_PATH))\n",
        "\n",
        "# 21-27 is good (~40 loss), trained with 1e-4. Trained on 80% of train_set\n",
        "# \n",
        "\n",
        "LOAD_PATH = PATH + \"/model-full_2022-12-09_21-27\"\n",
        "model = torch.load(LOAD_PATH)\n",
        "\n",
        "loss_by_batch, loss_by_epoch, score_by_batch = train_model(model, train_set, epochs=10000)\n",
        "\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
        "torch.save(model, PATH + \"/model-full_\" + dt_string)"
      ],
      "metadata": {
        "id": "wDOxkVvvN0ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD_PATH = PATH + \"/model-weights_2022-12-09_20-15\"\n",
        "# model = Wav442Letter(13, 29, bottleneck_size=500).to(DEVICE)\n",
        "# model.load_state_dict(torch.load(LOAD_PATH))\n",
        "\n",
        "# \"/model-full_2022-12-09_21-58\"\n",
        "\n",
        "LOAD_PATH = PATH + \"/model-full_2022-12-09_21-58\"\n",
        "model = torch.load(LOAD_PATH)\n",
        "\n",
        "eval_out, mean_wer, mean_lev_dist = eval_model(model, train_set)\n",
        "\n",
        "print(\"Mean WER: %.3f\" % mean_wer)\n",
        "print(\"Mean Distance: %.3f\\n\" % mean_lev_dist)\n",
        "print_eval(eval_out, n_samples=1)"
      ],
      "metadata": {
        "id": "d7-CreGKVxxn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}